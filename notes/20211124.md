## Properties of DP (cont from 2021-11-10)

a) Postprocessing preserves differential privacy
b) Sequential composition preserves differential privacy
   Let `M_1: X^n -> Y`, `M_2: X^n * Y -> Z`.
   That is `M_2` has access to both original dataset as well as `M_1`-sanitized
   dataset.
   Then, if `M_1` is `epsilon_1` DP, `M_2` is `epsilon_2` DP, then `M_2 . M_1`
   is `epsilon_1 + epsilon_2` DP.

### Proof sequential composition

For `z \in Z`, `X^n, X'^n` neighbouring datasets:
```
P[M_2(M_1(X^n), X^n) = z]
= Sum_{y \in Y}( P[M_z(y, X^n) = z] * P[M_1(X^n) = y] )
<= Sum_{y \in Y}( exp(epsilon_2) * P[M_2(y, X'^n) = z] * exp(epsilon_1) * P[M_1(X'^n) = y]
= exp(epsilon_1 + epsilon_2) * P[M_2(M_1(X'^n), X'^n) = z]
```

So the composition is `epsilon_1 + epsilon_2`-DP.

### Combination of `n`

### Group privacy

What if `b` positions (rather than 1) change from `X^n` to `X'^n`?
Then there exists a sequence `X^n = X_0 ~ X_1 ~ ... ~ X_b = X'^n`, 
such that any two consecutive data sets differ in one element only.

Then, for M with `epsilon`-DP, with `X^n` and `X'^n` differing in `b` entries,
then for all `Y` it holds that:
```
P(M(X^n) \in Y) <= exp(b * epsilon) * P(M(X'^n) \in Y)
```

That is `M` is `b * epsilon-DP` for sets differing in `b` elements.

# DP and private machine learning

- How does DP data inluence learning? (E.g. how much worse will ML be with DP
  data)
- How does ML on DP data impact the privacy of data?

ML extracts statistical evidence from data, and draws conclusions.
But: DP model guarantees that epsilon-DP dataset will not leak more privacy
than the stated epsilon, no matter the utilized technique.

## Example: Medical study considering attributes of patients and diseases

- Identifiers: Stripped from data
- Quasi identifiers: Patients' attributes, e.g. age, smoking, alcohol
  consumption, ...
- Sensitive attributes: Disease status

Study reveals correlation between QI of smoking and S of lung cancer.
But privacy preserved, because for every individual nothing is leaked about
their status.

## Example: Dataset

```
QI: 3  -2  9  2.718  -6
S:  4  -1  10 3.718  -5
```

ML predicts: `S = QI + 1`

Not a violation of privacy, because it's a generic truth which holds for
everyone. Compare: ML predicts that everyone has a mother and father.

See also: If any element of the data set was removed, the ML prediction would
stay the same.

Revealing the datapoint 2.718, which is identifying information (of Euler),
*is* the actual privacy violation as it is a full identifier.

## Example: Algorithm predicting pregnancies based on human behaviour

If algorithm able to predict pregnancy before human consciously aware, no
privacy violation. Reason: Prediction based on non-private information.

## Example: Netflix 'anonymized' view statistic correlation

Recall example where anonymized viewership data was correlated with e.g. movie
reviews to identify people's datasets.

Here, privacy violation as dataset itself not sufficiently anonymized.

## DP in practice

- Used by many online companies for e.g. 'privacy-aware' reporting of usage of
  software/devices.

### Practical concerns

#### Goals
- Learn how software / device is used by users
- Detect e.g. distribution of malware which changes observable behaviour

#### Concerns

1) Obtain settings, environment values, ... => Not numerical => Cannot work with naive Laplace noise
2) Values change little over time => Simple DP collection would reveal too much
3) ??? (Check notes)


#### Solution to 1: Bloom filters

Crash course bloom filters:
- B is k-bit vector initialized to `0^k`
- Turn string (or number) into a `k`-bit string, using `h` different hash
  functions `H_1, ..., H_k`
- Each hash function hashes input to one of the `k` bits of `B` (i.e. `H_i` has
  output `{0, ..., k-1}`)
- Set those bits of `b_i` of `B` to `1` where `H_j(v) = i` for all hash
  functions `H_j`
- To check whether an element was added to `B`, check if each of the bits `b_i
  = H_j(v)` for all hash functions `H_j` is set to 1
- One-sided error: If answer "no", then element was not added. If answer "yes",
  then element was either added, or false positive

Mind:
- B is then a bit vector, to which a randomised response can be applied.
- Since it does not change often, it cannot be sent repeatedly many times. If
  one did, the randomised noise could be filtered out, leaking the full B.
  If value must be reported repeatedly, randomized version must be stored
  locally, so same sanitized value can be reported on subsequent reports.
  => Memoization
  Specifically: One-off create DP version `B' = DP(B)` of `B`, and memoize it
  locally.
  When reporting, we then send `B'' = DP(B')` as part of the report.

##### RAPPOR

DP Bloom filters used by Google for reporting of Chrome settings.
- Daily reports, up to every 30 minutes
- 100 metrics, each is 2-DP
- Repeatedly collected until budget of ~4.4 reached
- Collecting from 14M clients, values started showing up in their bloom filters
  once shared by 14000 clients (0.1%).

#### Solution to 3: Efficient data collection

- Each user reports `X_i \in [0, m]` for `i = 1 ... n`
- Local Laplace mechanism gives `Y_i = X_i + Lap(m/epsilon)`
- `Y_i` fed to central collector

Idea: Report just one bit, such that probabilistic estimator possible on side
of collector.
This bit computed as:
  - `1 with probability 1/(exp(epsilon) + 1) * X_i / m * (exp(e) - 1) / (exp(e) + 1)`
  - `0` otherwise

=> Details: Microsoft paper
