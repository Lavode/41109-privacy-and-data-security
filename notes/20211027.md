# Data anonymization

Recall:
- Anonymization eliminates personal data
- Anonymized data thus not subject to privacy laws

- Publish anonymized data: Cannot be associated to individuals
- Presence or absence of individual in anonymized data cannot be detected

Tradeoff:
- Full dataset has complete utility
- Anonymized dataset has no utility

- Measure for privacy or anonymity needed (also for utility, but that's a data
  science topic)

Anonymization is difficult:
- Correlation with additional / background data
- Predicting how much related information exists seems impossible

Famous examples of correlation:
- Anonymized US healthcare records, correlated with voter records, identified
  government employees
- Pseudonymized Netflix movie reviews, correlated with e.g. IMDB data,
  identified individual users

## K-anonymity

- Dataset in table, with attribute set `A`
- Each row (entry) in a table is personal data (record)
  E.g:
  ```
  name,   firstname, PLZ,  points in series, operating system
  Cachin, Christian, 8500, 9,                Linux
  ```
- Partition attribute set `A` into classes:
  - Sensitive attributes `S`, attributes which an adversary wants to learn
    about for each individual. E.g. operating system
  - Identifiers `I`: Identify individual. E.g. name and first name.
  - Quasi-identiiers `QI`: Feature of data which may (or may help to) identify
    individual, e.g. birth date

- Here: `A = I U QI U S`

### Definition

Dataset is k-anonymous if each partition of the dataset by combination of
quasi-identifiers must have at least `k` members.

- Identifiers are removed beforehand
- The `k` elements must *not* be distinct in terms of sensitive information,
  there simply must be `>= k` elements.
  Having different sensitive information: Diversity, not anonymity.


### Techniques

#### Supression

- Remove infrequent elements of the quasi-identifier

#### Generalization

- Replace QI values by more general values
- For numerical data: Use intervals
  - Intervals may overlap if needed, but better for utility if no overlap
- For categorical data: Hierarchy of values needed (specific knowledge of data)
  to generalize
  E.g. Jobs can be organized into hierarchical categories, dito operating
  systems

### Problems with this notion

- Background knowledge: Can allow correlation, still. E.g if knowing that person
  lives in PLZ range, and has a mobile, can potentially deduce which mobile OS.
- Homogeneity attack: If all members of partition have same sensitive data, then
  lack of diversity allows learning of individual's sensitive information
  still.

## L-diversity

### Def

An equivalence class is l-diverse if it contains at least `l`
"well-represented" values for the sensitive attributes.

A dataset is l-diverse if each of its equivalence classes (= partitions by
quasi-identifiers) is l-diverse.

### "Well-represented"

Multiple ways to define:
- Distinct l diversity: Sensitive attributes take on at least `l` different
  values
- Probabilistic diversity: Proportion of each attribute is at most `1/l`
  - Stronger requirement. E.g dataset (1, 1, 1, ..., 1, 2, 3) is 3-diverse
    based on the first, but not on the second criteria

### Problems

- Similarity of sensitive data: If whole equivalence class used Windows or
  Linux, we'd still learn that all of them are on a desktop OS.
- Skewed data set: If sensitive data has one value which is unlikely - e.g.
  medical test (HIV, cancer, ...) - yet one equivalence class has a
  significantly higher ratio of positives (by pure chance) - then an individual
  being part of an equivalence class changes the inormation compared to the a
  priori one.
  - Summing up: Classes not representative of the full dataset pose an issue


