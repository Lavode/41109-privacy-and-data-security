# Epsilon-Closeness

Given dataset, set of sensitive attributes `S`.
Here: Only one sensitive attribute, for simplicity: `S = {s}`

- Initial (a priori) assumption::
  - Random variable A over S: `Sum of P(A = s) for s in S = 1`
  - E.g. uniform distribution over operating system

- Completely generalized dataset statistics Q over S:
  - `P_Q = P(Q = s) = (number of entries with S = s) / (number of total entries)`
  - Empirical distribution of sensitive attribute

- When observer learns values of QI in one partition, only one equivalence
  class C remains.
  - Goal: Information the observer now has should be as close as possible to the
    a priori one.
  - Let L denote the attribute S restricted to C, this is the leaked information
  - `P_L = P(L = s) = (number of entries in C with S = s) / (number of entries in C)`

## Epsilon-closeness

An equivalence class C is epsilon-close to the full dataset, if:
`D(P_L, P_Q) <= epsilon`

A dataset has epsilon-closeness if all of its equivalence classes are
epsilon-close to the dataset.

D is a distance "measure" (not necessarily the math sense) between two
probability distributions. Examples:
- L2 norm
- L1 norm
- Variational distance: `d(P_L, P_Q) = 1/2 * Sum (for s in S) |P_L(s) - P_Q(s)|`
- Kullback-Leibler divergence: `Sum (for s in S) P_L(s) * log(P_L(s) / P_Q(s))`

### n-epsilon-closeness

For large dataset: Can be desireable to use an n-subset of the full dataset for
the generalized statistics `P_Q`.

Def:
Equivalence class C is (n-epsilon)-close to the full dataset if there exists a
subset `M` of the dataset, with `|M| >= n`, and `D(P_L, P_{Q | M}) <= epsilon`

(Notation: `P_{Q | M}` dataset statistics Q limited to subset M.)

A partitioning C is (n, epsilon)-close to the full dataset iff there exists a
subset `M` of the dataset, with `|M| >= n`, such that **each** equivalence
class `L` satisfies `D(P_L, P_{Q | M}) <= epsilon`

Uses any subset of sufficient sie for reference to hide the disclosed
distribution among this set. [? - verbatim copy of sentence]

# Recap

- So far: Participants forward data to trusted aggregator,whc  receives all
  information and sanitizes it, producing sanitzed dataset.
- Public consumes sanitized statistic. Untrusted participants.
- Next: Allow participants to already sanitize their data.

# Differential privacy

## Randomised response

- N persons, each has a sensitive value `x_i \in {0, 1}`
  Suppose each `x_i ~ Bernoulli(p)`, that is `P(x_i = 1) = p` for all i

- `p` unknown. Observer wants to estimate p, but must not violate privacy of
  participants

Example: n = 18, 1 observer. "Have you ever cheated in an exam?"
  - 1 = yes, 0 = no
Idea: Add randomization to add deniability
  - Flip bit. If 0 => respond with actual answer. If 1 => respond with random (new bit) answer.

Formal:
- `P_i` sends `y_i = (x_i with probability alpha, r_i with probability (1 - alpha))`
    - alpha = 0 is full privacy, alpha = 1 is full utility. Allows privacy /
      utility tradeoff
- Where `r_i` uniform random bit
- Observer estimates `p`:
  - `Y = sum y_i`
  - `P(x_i = 1) = p, E[x_i] = p`
  - `E[y_i] = alpha * E[x_i] + (1 - alpha) * E[y_i] = alpha * p + (1 - alpha) * 1/2`
  - `p = 1 / alpha  * (E[y_i] - (1 - alpha) * 1/2)`
  - Estimator of p `p-hat`:
```
  p-hat = 1/alpha * (Y / n - (1 - alpha) / 2) 
        = 1/alpha * ((Sum of y_i) / n - (1 - alpha) / 2)
        = 1 / (alpha * n) * (sum of y_i) - (1 - alpha) / (2 alpha)
```

- Experiment with alpha = 1/2: 9 times '0', 9 times '1':
  - p-hat = 1 / 2

### Accuracy of this estimator

```
Var(p-hat) = Var(1 / (alpha *n) * (sum of y_i) - (1 - alpha) / 2 alpha)`
           = 1 / (alpha^2 * n^2) * (sum of Var(y_i)) 
           # Variance(y_i) <= 1
           <= 1 / (alpha^2 * n) 
```

Using Chebyshev:

```
P[|p-hat - p| >= epsilon] <= Var[p-hat] / epsilon^2
```

So linear in the number of samples. For fixed privacy / usability tradeoff,
estimator becomes more accurate in proportion 1/n.

## Defining differential privacy (DP)

Given:
- `n` data values `x_1, ..., x_n` corresponding to secret values of `n` individuals
  - `X = {0, 1}` or `X = natural numbers`
- An algorithm `M: X^n -> Y` sanitizes a vector `x^n \in X^n` and outputs `y \in Y`
- M must be randomized
- DP is feature of algorithm (not of dataset)

### Def neighbouring datasets

Two datasets `x^n, x'^n` are neighbouring if they differ in exactly one
element.
Notation: `x^n ~ x'^n`

### Def differential privacy

A (randomized) algorithm `M: X^n -> Y` is epsilon-differentially private if:

For all `Y' \subset Y`, and for all `x^n, x'^n` such that `x^n ~ x'^n`:
```
P[M(x^n) \in Y'] <= e^{epsilon} * P[M(x'^n) \in Y']
```
